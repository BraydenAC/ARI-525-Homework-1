import os
import collections
from gensim.corpora import Dictionary
from scipy import sparse
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import pyLDAvis.gensim
from gensim.models import LdaModel
from gensim.matutils import Sparse2Corpus

#DISCLAIMER: Certain chunks of code in this program were made by chatGPT

def preprocess_text(inputString, lowercase, lemma, cleanPunctuation, stopword):
    newText = inputString
    if lowercase:
        #Do Lowercasing
        newText = newText.lower()

    #Tokenize the input
    tokenizedString = []
    for line in newText.splitlines():
        #Stores each line from input string as a tokenized array form of the original
        tokenizedString.append(line.split())
    newText = tokenizedString

    #Stopword Removal
    if stopword:
        #Load external stopword list
        mystops = stopwords.words('english')
        #From class colab, apply stopwords and tokenize
        mystops = set(mystops)
        newText = [[tok for tok in row if tok not in mystops] for row in newText]

    #Lemmatizer
    if lemma:
        #Lemmatize with NLTK
        lemmatizer = WordNetLemmatizer()
        newText = [[lemmatizer.lemmatize(word) for word in row] for row in newText]

    if cleanPunctuation:
        #Generated by Chatgpt
        newText = [[re.sub(r'[^\w\s]', '', word) for word in row] for row in newText]

    #Row below generated by ChatGPT, removes empty string elements
    newText = [[element for element in row if element] for row in newText if row and any(row)]
    return newText

#   Use sparse matrix described in homework prompt?
folders = ["LeoTolstoy_Chapters", "HGWells_Chapters"]
file_data = []
files = []
for folder in folders:
    files.extend([f"{folder}/{f}" for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))])

for file_name in files:
    #Import file
    with open(file_name, "r", encoding="utf-8") as file:
        stringInput = file.read()

    #Preprocess the text: make sure to change settings here with booleans
    print(f"processing {file_name}...")
    processed_chapter = preprocess_text(stringInput, lowercase=True, lemma=True, cleanPunctuation=True, stopword=True)

    #convert the chapter into a default dictionary of tokens and their counts like in homework 0
    token_list = collections.defaultdict(int)

    for line in processed_chapter:
        for token in line:
            token_list[token] += 1
    file_data.append(token_list)

# Create a token-to-index mapping(directly by chatgpt)
unique_tokens = sorted(set(token for doc in file_data for token in doc))
token_index = {token: i for i, token in enumerate(unique_tokens)}

#Create an index-to-token flip mapping(from colab)
reverse_token_index = {id: tok for tok, id in token_index.items()}

#Store as sparse, pulled from class colab
data, row_indices, col_indices = [], [], []
#Iterates through each defaultdict within file_data, and splits file_data into current list number and contained dict
for row_idx, document_dict in enumerate(file_data):
    #Iterates through each default_dict, split into key and value
    for token, count in document_dict.items():
        #row stores document, col stores tokens?
        row_indices.append(row_idx)
        col_indices.append(token_index[token])
        #data stores
        data.append(count)

mat = sparse.csr_matrix((data, (row_indices, col_indices)), shape=(len(files), len(unique_tokens)), dtype='float64')

#Initialize a pandas dataframe, with chapter names as columns and individual token types as rows
# BoW_dataframe = pd.DataFrame(mat.toarray(), index=files, columns=unique_tokens)

# #Iterate through all default dicts, storing ints in accordance with keys
# print("Saving data to DataFrame...")
# for chapter, tokens in enumerate(file_data):
#     for token, count in tokens.items():
#         BoW_dataframe.loc[token, files[chapter]] = count

# #Store finished dataframe in csv
# print("Saving data...")
# BoW_dataframe.to_csv("combo.csv", index=True)
# print("Done!")
#-------------------------------------------------------------------------------------------------------

#pulled from lecture, preps file data in model's preferred format
corpus = Sparse2Corpus(mat, documents_columns=False)

#Initialize a gensim appropriate token-to-id index
dictionary = Dictionary([[word for word, count in doc.items()] for doc in file_data])
dictionary.filter_extremes(no_below=1, no_above=1.0, keep_n=None)

#Train model
print("Training Model...")
lda_model = LdaModel(corpus=corpus, num_topics=10, id2word=dictionary)

#Display Results
prepared_data = pyLDAvis.gensim.prepare( lda_model, corpus, dictionary, mds='mmds')
pyLDAvis.save_html(prepared_data, "LDAnoStopwords.html")
