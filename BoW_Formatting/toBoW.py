import os
import numpy as np
import collections
import pandas as pd
from scipy import sparse
import nltk
import re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

def preprocess_text(inputString, lowercase, lemma, cleanPunctuation):
    newText = inputString
    if lowercase:
        #Do Lowercasing
        newText = newText.lower()

    #Tokenize the input
    x = 0
    tokenizedString = []
    for line in newText.splitlines():
        #Stores each line from input string as a tokenized array form of the original
        tokenizedString.append(line.split())
        x += 1
    newText = tokenizedString

    #Lemmatizer
    if lemma:
        #Lemmatize with NLTK
        lemmatizer = WordNetLemmatizer()
        newText = [[lemmatizer.lemmatize(word) for word in row] for row in newText]

    if cleanPunctuation:
        #Generated by Chatgpt
        newText = [[re.sub(r'[^\w\s]', '', word) for word in row] for row in newText]

    #Row below generated by ChatGPT, removes empty string elements
    newText = [[element for element in row if element] for row in newText if row and any(row)]
    return newText

#   Use sparse matrix described in homework prompt?
folder = "LeoTolstoy_Chapters"
files = [f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))]
file_data = collections.defaultdict(lambda: collections.defaultdict(int))


for file_name in files:
    #Import file
    with open(folder + "/" + file_name, "r") as file:
        stringInput = file.read()

    #Preprocess the text: make sure to change settings here with booleans
    print(f"processing {file_name}...")
    processed_chapter = preprocess_text(stringInput, lowercase=True, lemma=True, cleanPunctuation=True)

    #convert the chapter into a default dictionary of tokens and their counts like in homework 0
    token_list = collections.defaultdict(int)

    for line in processed_chapter:
        for token in line:
            token_list[token] += 1
    file_data[file_name] = token_list

# Collect all unique chapter keys
unique_keys = set()
for inner_dict in file_data.values():
    unique_keys.update(inner_dict.keys())

# Convert to a list
unique_keys_list = list(unique_keys)

#Initialize a pandas dataframe, with chapter names as columns and individual token types as rows
BoW_dataframe = pd.DataFrame(columns=file_data.keys(), index=unique_keys_list).fillna(0)

#Iterate through all default dicts, storing ints in accordance with keys
print("Saving data to DataFrame...")
for chapter, tokens in file_data.items():
    for token, count in tokens.items():
        BoW_dataframe.loc[token, chapter] = count

#Store finished dataframe in csv
print("Saving data...")
BoW_dataframe.to_csv("BoW_Tolstoy.csv", index=True)
print("Done!")