import os
import collections
from gensim.corpora import Dictionary
from scipy import sparse
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import pyLDAvis.gensim
from gensim.models import LdaModel
from gensim.matutils import Sparse2Corpus

#WARNING:Due to some issue with the prepare function, this code WILL freeze then crash your computer if you run it
#        I have left the code here as evidence of effort.

#DISCLAIMER: Certain chunks of code in this program were made by chatGPT

def preprocess_text(inputString, lowercase, lemma, stopword, cleanPunctuation):
    newText = inputString
    if lowercase:
        #Do Lowercasing
        newText = newText.lower()

    #Tokenize the input
    tokenizedString = []
    for line in newText.splitlines():
        #Stores each line from input string as a tokenized array form of the original
        tokenizedString.append(line.split())
    newText = tokenizedString

    #Lemmatizer
    if lemma:
        #Lemmatize with NLTK
        lemmatizer = WordNetLemmatizer()
        newText = [[lemmatizer.lemmatize(word) for word in row] for row in newText]

    #Stopword Removal
    if stopword:
        #Load external stopword list
        mystops = stopwords.words('english')
        #From class colab, apply stopwords and tokenize
        mystops = set(mystops)
        newText = [[tok for tok in row if tok not in mystops] for row in newText]

    if cleanPunctuation:
        #Generated by Chatgpt
        newText = [[re.sub(r'[^\w\s]', '', word) for word in row] for row in newText]

    #Row below generated by ChatGPT, removes empty string elements
    newText = [[element for element in row if element] for row in newText if row and any(row)]
    return newText

#Initialize variables
vocab = set()
tok_to_id, row_to_doc = {}, {}
current_row = 0
data, row_indices, col_indices = [], [], []
document_dicts = []
storage_type = "count"
unique_token_count = 0

#   Use sparse matrix described in homework prompt?
folders = ["LeoTolstoy_Chapters", "HGWells_Chapters"]
file_data = []
files = []
for folder in folders:
    files.extend([f"{folder}/{f}" for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))])

#Preprocess text
for file_name in files:
    with open(file_name, "r", encoding="utf-8") as file:
        stringInput = file.read()
    processed_chapters = preprocess_text(stringInput, lowercase=True, lemma=True, stopword=False, cleanPunctuation=True)

    #Tokenize(modified from colab)
    document_dict = collections.defaultdict(int)
    for line in processed_chapters:
        for element in line:
            if element not in tok_to_id:
                tok_to_id[element] = unique_token_count
                unique_token_count += 1
                vocab.add(element)

                token_id = tok_to_id[element]
                document_dict[token_id] += 1

            document_dicts.append(document_dict)
            row_to_doc[current_row] = file_name
            current_row += 1

#Get flipped token:index mapping
id_to_tok = {id:tok for tok,id in tok_to_id.items()}

#Store data in sparse matrix(from colab)
print("Storing data in matrix...")
for row_idx, document_dict in enumerate(document_dicts):
    for token_id, count in document_dict.items():
        row_indices.append(row_idx)
        col_indices.append(token_id)
        data.append(count)
mat = sparse.csr_matrix((data, (row_indices, col_indices)), shape=(len(document_dicts), unique_token_count), dtype='float64')

#Binary or Count-based storage
if storage_type == "count":
    pass
if storage_type == "binary":
    mat = (mat > 0).astype(float) if not sparse else mat.sign()

#Create LDA model
corpus = Sparse2Corpus(mat, documents_columns=False)
print("Training Model...")
lda_model = LdaModel(corpus=corpus, num_topics=5, id2word=id_to_tok)

#Prepare data(from chatgpt)
print("Preparing data")
id_to_tok_dict = Dictionary(processed_chapters)  # Use original tokenized text
prepared_data = pyLDAvis.gensim.prepare(lda_model, corpus, id_to_tok_dict, mds='mmds', sort_topics=False)

print("Saving to LDA.html")
pyLDAvis.save_html(prepared_data, "LDA.html")
print("Done!")



